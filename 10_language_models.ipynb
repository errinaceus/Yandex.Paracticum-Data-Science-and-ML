{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project for Wikishop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Wikishop online store is launching a new service. Now users can edit and add product descriptions, just like in wiki communities. That is, clients suggest their edits and comment on the changes of others. The store needs a tool that will look for toxic comments and send them for moderation.\n",
    "\n",
    "Train the model to classify comments into positive and negative. At your disposal is a set of data with markings about the toxicity of edits.\n",
    "\n",
    "Build a model with a quality metric value *F1* of at least 0.75.\n",
    "\n",
    "**Instructions for completing the project**\n",
    "\n",
    "1. Download and prepare data.\n",
    "2. Train different models.\n",
    "3. Draw conclusions.\n",
    "\n",
    "It is not necessary to use *BERT* to complete the project, but you can try.\n",
    "\n",
    "**Description of data**\n",
    "\n",
    "The data is in the file `toxic_comments.csv`. The *text* column in it contains the text of the comment, and *toxic* is the target attribute."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from lightgbm import LGBMClassifier\n",
    "from nltk.corpus import stopwords as nltk_stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "from sklearn.model_selection import train_test_split,GridSearchCV\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from tqdm.notebook import tqdm\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 159292 entries, 0 to 159291\n",
      "Data columns (total 3 columns):\n",
      " #   Column      Non-Null Count   Dtype \n",
      "---  ------      --------------   ----- \n",
      " 0   Unnamed: 0  159292 non-null  int64 \n",
      " 1   text        159292 non-null  object\n",
      " 2   toxic       159292 non-null  int64 \n",
      "dtypes: int64(2), object(1)\n",
      "memory usage: 3.6+ MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                               text  toxic\n",
       "0           0  Explanation\\nWhy the edits made under my usern...      0\n",
       "1           1  D'aww! He matches this background colour I'm s...      0\n",
       "2           2  Hey man, I'm really not trying to edit war. It...      0\n",
       "3           3  \"\\nMore\\nI can't make any real suggestions on ...      0\n",
       "4           4  You, sir, are my hero. Any chance you remember...      0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = pd.read_csv('datasets/toxic_comments.csv')\n",
    "display(data.info())\n",
    "display(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are no gaps in the data; we will lemmatize the text and clean it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.7.1\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /Users/artembonchuk/anaconda3/lib/python3.11/site-packages (from en-core-web-sm==3.7.1) (3.7.4)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /Users/artembonchuk/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /Users/artembonchuk/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/artembonchuk/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/artembonchuk/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/artembonchuk/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /Users/artembonchuk/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.3)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /Users/artembonchuk/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /Users/artembonchuk/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /Users/artembonchuk/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /Users/artembonchuk/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.3.4)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /Users/artembonchuk/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.9.4)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /Users/artembonchuk/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (5.2.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/artembonchuk/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.65.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/artembonchuk/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.31.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /Users/artembonchuk/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.7.0)\n",
      "Requirement already satisfied: jinja2 in /Users/artembonchuk/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.2)\n",
      "Requirement already satisfied: setuptools in /Users/artembonchuk/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (68.0.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/artembonchuk/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (23.0)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /Users/artembonchuk/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /Users/artembonchuk/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.24.3)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /Users/artembonchuk/anaconda3/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.18.1 in /Users/artembonchuk/anaconda3/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.18.1)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /Users/artembonchuk/anaconda3/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.7.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/artembonchuk/anaconda3/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/artembonchuk/anaconda3/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/artembonchuk/anaconda3/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/artembonchuk/anaconda3/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2023.7.22)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /Users/artembonchuk/anaconda3/lib/python3.11/site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /Users/artembonchuk/anaconda3/lib/python3.11/site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /Users/artembonchuk/anaconda3/lib/python3.11/site-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.0.4)\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /Users/artembonchuk/anaconda3/lib/python3.11/site-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/artembonchuk/anaconda3/lib/python3.11/site-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.1.1)\n",
      "Installing collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-3.7.1\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!python3 -m spacy download en_core_web_sm\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_text(text):\n",
    "    text = text.lower()\n",
    "    doc = nlp(text)\n",
    "    lemm_text = \" \".join([token.lemma_ for token in doc])\n",
    "    cleared_text = re.sub(r'[^a-zA-Z]', ' ', lemm_text) \n",
    "    return \" \".join(cleared_text.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text\n",
      "0  The striped bats are hanging on their feet for...\n",
      "1      you should be ashamed of yourself went worked\n",
      "0    the stripe bat be hang on their foot for good\n",
      "1        you should be ashamed of yourself go work\n",
      "Name: text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "#Let's check that lemmatization works correctly\n",
    "sentence1 = \"The striped bats are hanging on their feet for best\"\n",
    "sentence2 = \"you should be ashamed of yourself went worked\"\n",
    "df_my = pd.DataFrame([sentence1, sentence2], columns = ['text'])\n",
    "print(df_my)\n",
    "\n",
    "\n",
    "print(df_my['text'].apply(lemmatize_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e097dbb7c0a440b84225b89d633ff09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/159292 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data['lemm_text'] = data['text'].progress_apply(lemmatize_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's explore class balance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    143106\n",
       "1     16186\n",
       "Name: toxic, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['toxic'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a strong imbalance of classes, which will need to be taken into account when building models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/artembonchuk/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "#select the target feature and divide the data into training and training samples\n",
    "target=data['toxic']\n",
    "features=data['lemm_text']\n",
    "#Let's allocate 20% of the data to the test sample\n",
    "features_train, features_test, target_train, target_test = train_test_split(\n",
    "    features, target, test_size=0.2, random_state=12345)\n",
    "#From the remaining data, we will allocate 25% of the data to the validation sample\n",
    "features_train, features_valid, target_train, target_valid = train_test_split(\n",
    "    features_train, target_train, test_size=0.25, random_state=12345)\n",
    "\n",
    "nltk.download('stopwords')\n",
    "stopwords = set(nltk_stopwords.words('english'))\n",
    "count_tf_idf = TfidfVectorizer(stop_words=stopwords) \n",
    "count_tf_idf.fit(features_train)\n",
    "features_train = count_tf_idf.transform(features_train)\n",
    "features_valid = count_tf_idf.transform(features_valid)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train a logistic regression model taking into account class imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression on validation set: 0.7408412483039348\n"
     ]
    }
   ],
   "source": [
    "model=LogisticRegression(random_state=12345,class_weight='balanced')\n",
    "model.fit(features_train,target_train)\n",
    "predictions = model.predict(features_valid)\n",
    "\n",
    "f1 = f1_score(target_valid, predictions)\n",
    "print('Logistic regression on validation set:', f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to select hyperparameters, namely the inverse force of regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/artembonchuk/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/artembonchuk/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/artembonchuk/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression on validation set: 0.7566007791083538\n",
      "10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/artembonchuk/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "best_score = 0\n",
    "best_c = 0\n",
    "for i in [1, 10, 50, 100, 200]:\n",
    "    model = LogisticRegression(C=i,random_state=12345,class_weight='balanced')\n",
    "    model.fit(features_train, target_train)\n",
    "    prediction = model.predict(features_valid)\n",
    "    f1=f1_score(target_valid,prediction)\n",
    "    if f1 > best_score:\n",
    "        best_score = f1\n",
    "        best_c = i\n",
    "print('Logistic regression on validation set:',best_score)\n",
    "print(best_c)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the value C=10 the result was better"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train the LightGBM model taking into account class imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LightGBM on validation set: 0.7391365888181174\n"
     ]
    }
   ],
   "source": [
    "model_gbm=LGBMClassifier(random_state=12345,class_weight='balanced')\n",
    "model_gbm.fit(features_train,target_train)\n",
    "predictions = model_gbm.predict(features_valid)\n",
    "\n",
    "f1 = f1_score(target_valid, predictions)\n",
    "print('LightGBM on validation set:', f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is not good enough, let’s try to select hyperparameters, namely the number of trees (default 100) and the number of “leaves” (default 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_gbm=LGBMClassifier(random_state=12345)\n",
    "params = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'num_leaves': [21, 41],\n",
    "}\n",
    "# using GridSearchCV \n",
    "grid_gbm = GridSearchCV(model_gbm,\n",
    "                        params,\n",
    "                        n_jobs=-1,\n",
    "                        scoring='f1')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_score: 0.7715078293138841\n",
      "best_params: {'n_estimators': 300, 'num_leaves': 41}\n"
     ]
    }
   ],
   "source": [
    "grid_gbm.fit(features_train, target_train)\n",
    "\n",
    "grid_gbm_best_score = grid_gbm.best_score_ \n",
    "grid_gbm_best_params = grid_gbm.best_params_\n",
    "print(f'best_score: {grid_gbm_best_score}')\n",
    "print(f'best_params: {grid_gbm_best_params}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will train a random forest model taking into account class imbalance, and also select hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random forest model on validation set: 0.3775293229988339\n",
      "14\n",
      "201\n"
     ]
    }
   ],
   "source": [
    "\n",
    "best_score = 0\n",
    "best_depth = 0\n",
    "best_est = 0\n",
    "for depth in range(2, 15, 2):\n",
    "    for est in range(51, 251, 50):\n",
    "        model = RandomForestClassifier(n_estimators=est, max_depth=depth, random_state=12345,class_weight='balanced')\n",
    "        model.fit(features_train, target_train)\n",
    "        prediction = model.predict(features_valid)\n",
    "        f1=f1_score(target_valid,prediction)\n",
    "        if f1 > best_score:\n",
    "            best_score = f1\n",
    "            best_depth = depth\n",
    "            best_est = est\n",
    "print('Random forest model on validation set:',best_score)\n",
    "print(best_depth)\n",
    "print(best_est)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The LGBMClassifier model shows the best results, let’s check it on a test sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LightGBM on test dataset: 0.7637448501207558\n"
     ]
    }
   ],
   "source": [
    "#vectorize the text using tf_idf trained on the training set\n",
    "features_test = count_tf_idf.transform(features_test)\n",
    "#train and test the model taking into account the selected hyperparameters and class imbalance\n",
    "model_gbm=LGBMClassifier(n_estimators=300, num_leaves=41, random_state=12345,class_weight='balanced')\n",
    "model_gbm.fit(features_train,target_train)\n",
    "predictions = model_gbm.predict(features_test)\n",
    "\n",
    "f1 = f1_score(target_test, predictions)\n",
    "print('LightGBM on test dataset:', f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Сonclusions\n",
    " \n",
    "   After preprocessing the text, the LogisticRegression, LightGBMClassifier and RandomForestClassifier models were trained. After selecting hyperparameters, the LogisticRegression and LightGBMClassifier models allowed us to achieve an F1 quality metric of at least 0.75 on the validation set. During data preprocessing, a strong class imbalance was identified, which was taken into account when building models. Since LGBMClassifier showed the best result and its work was tested on the test set, the model showed a result of F1 = 0.763 (0.772 on the validation set).\n",
    "\n",
    "Result: the LGBMClassifier model can be recommended as the best model for classifying comments, taking into account class imbalance, which gives the result F1 = 0.763."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "ExecuteTimeLog": [
   {
    "duration": 1998,
    "start_time": "2023-07-27T08:43:40.700Z"
   },
   {
    "duration": 160,
    "start_time": "2023-07-27T08:43:46.145Z"
   },
   {
    "duration": 2355,
    "start_time": "2023-07-27T08:44:05.997Z"
   },
   {
    "duration": 10,
    "start_time": "2023-07-27T09:47:42.971Z"
   },
   {
    "duration": 184,
    "start_time": "2023-07-27T09:47:54.724Z"
   },
   {
    "duration": 13395,
    "start_time": "2023-07-27T09:47:59.812Z"
   },
   {
    "duration": 46124,
    "start_time": "2023-07-27T09:48:13.209Z"
   },
   {
    "duration": 42353,
    "start_time": "2023-07-27T09:51:08.707Z"
   },
   {
    "duration": 28,
    "start_time": "2023-07-27T09:58:27.402Z"
   },
   {
    "duration": 10,
    "start_time": "2023-07-27T09:58:48.441Z"
   },
   {
    "duration": 4,
    "start_time": "2023-07-27T09:59:13.243Z"
   },
   {
    "duration": 1264604,
    "start_time": "2023-07-27T09:59:16.942Z"
   },
   {
    "duration": 807208,
    "start_time": "2023-07-27T10:35:01.674Z"
   },
   {
    "duration": 49307,
    "start_time": "2023-07-27T10:57:26.934Z"
   },
   {
    "duration": 6,
    "start_time": "2023-07-27T11:00:56.543Z"
   },
   {
    "duration": 12,
    "start_time": "2023-07-27T11:04:54.954Z"
   },
   {
    "duration": 13,
    "start_time": "2023-07-27T11:05:07.316Z"
   },
   {
    "duration": 16,
    "start_time": "2023-07-27T11:05:11.454Z"
   },
   {
    "duration": 13,
    "start_time": "2023-07-27T11:11:40.666Z"
   },
   {
    "duration": 121964,
    "start_time": "2023-07-27T11:12:47.101Z"
   },
   {
    "duration": 15,
    "start_time": "2023-07-27T11:15:12.146Z"
   },
   {
    "duration": 52,
    "start_time": "2023-07-27T11:15:19.013Z"
   },
   {
    "duration": 52895,
    "start_time": "2023-07-27T11:15:23.170Z"
   },
   {
    "duration": 54310,
    "start_time": "2023-07-27T11:17:13.163Z"
   },
   {
    "duration": 891344,
    "start_time": "2023-07-27T11:19:00.546Z"
   },
   {
    "duration": 1501205,
    "start_time": "2023-07-27T11:41:27.458Z"
   },
   {
    "duration": 5,
    "start_time": "2023-07-27T12:06:28.667Z"
   },
   {
    "duration": 372,
    "start_time": "2023-07-28T14:20:01.138Z"
   },
   {
    "duration": 1700,
    "start_time": "2023-07-28T14:20:06.911Z"
   },
   {
    "duration": 2396,
    "start_time": "2023-07-28T14:20:11.820Z"
   },
   {
    "duration": 19247,
    "start_time": "2023-07-28T14:20:17.685Z"
   },
   {
    "duration": 5,
    "start_time": "2023-07-28T14:20:36.934Z"
   },
   {
    "duration": 46,
    "start_time": "2023-07-28T14:20:41.054Z"
   },
   {
    "duration": 55674,
    "start_time": "2023-07-28T14:20:43.701Z"
   },
   {
    "duration": 96,
    "start_time": "2023-07-29T10:37:48.641Z"
   },
   {
    "duration": 1874,
    "start_time": "2023-07-29T10:37:59.860Z"
   },
   {
    "duration": 5102,
    "start_time": "2023-07-29T10:38:04.044Z"
   },
   {
    "duration": 10121,
    "start_time": "2023-07-29T10:38:12.803Z"
   },
   {
    "duration": 6,
    "start_time": "2023-07-29T10:38:27.960Z"
   },
   {
    "duration": 13233,
    "start_time": "2023-07-29T10:38:30.835Z"
   },
   {
    "duration": 47352,
    "start_time": "2023-07-29T10:38:53.450Z"
   },
   {
    "duration": 47738,
    "start_time": "2023-07-29T10:39:40.804Z"
   },
   {
    "duration": 53039,
    "start_time": "2023-07-29T10:40:28.544Z"
   },
   {
    "duration": 1487131,
    "start_time": "2023-07-29T10:41:33.053Z"
   },
   {
    "duration": 50959,
    "start_time": "2023-07-29T11:34:40.722Z"
   },
   {
    "duration": 174124,
    "start_time": "2023-07-29T11:35:31.684Z"
   },
   {
    "duration": 75,
    "start_time": "2023-07-29T11:51:14.282Z"
   },
   {
    "duration": 11,
    "start_time": "2023-07-29T11:51:25.542Z"
   },
   {
    "duration": 29,
    "start_time": "2023-07-29T11:51:31.358Z"
   },
   {
    "duration": 23,
    "start_time": "2023-07-29T11:51:55.159Z"
   },
   {
    "duration": 10,
    "start_time": "2023-07-29T11:52:07.506Z"
   },
   {
    "duration": 9,
    "start_time": "2023-07-29T11:52:14.142Z"
   },
   {
    "duration": 858183,
    "start_time": "2023-07-29T11:52:24.116Z"
   },
   {
    "duration": 0,
    "start_time": "2023-07-29T12:06:42.301Z"
   },
   {
    "duration": 266602,
    "start_time": "2023-07-29T12:08:11.104Z"
   },
   {
    "duration": 69,
    "start_time": "2023-07-29T12:12:37.714Z"
   },
   {
    "duration": 1063112,
    "start_time": "2023-07-29T12:12:37.787Z"
   },
   {
    "duration": 7,
    "start_time": "2023-07-29T12:30:30.336Z"
   },
   {
    "duration": 190870,
    "start_time": "2023-07-29T12:32:42.454Z"
   },
   {
    "duration": 3,
    "start_time": "2023-07-29T12:36:10.226Z"
   },
   {
    "duration": 11,
    "start_time": "2023-07-29T12:36:14.495Z"
   },
   {
    "duration": 4,
    "start_time": "2023-07-29T12:40:58.099Z"
   },
   {
    "duration": 9,
    "start_time": "2023-07-29T12:41:02.191Z"
   },
   {
    "duration": 4,
    "start_time": "2023-07-29T12:42:35.221Z"
   },
   {
    "duration": 7,
    "start_time": "2023-07-29T12:42:39.050Z"
   },
   {
    "duration": 5,
    "start_time": "2023-07-29T12:43:12.410Z"
   },
   {
    "duration": 7,
    "start_time": "2023-07-29T12:43:15.808Z"
   },
   {
    "duration": 3,
    "start_time": "2023-07-29T12:43:42.803Z"
   },
   {
    "duration": 1616,
    "start_time": "2023-07-29T12:43:44.859Z"
   },
   {
    "duration": 4,
    "start_time": "2023-07-29T12:44:17.151Z"
   },
   {
    "duration": 9,
    "start_time": "2023-07-29T12:44:27.291Z"
   },
   {
    "duration": 66,
    "start_time": "2023-07-29T12:45:20.336Z"
   },
   {
    "duration": 20466,
    "start_time": "2023-07-29T12:47:47.940Z"
   },
   {
    "duration": 283,
    "start_time": "2023-07-29T12:48:49.042Z"
   },
   {
    "duration": 4,
    "start_time": "2023-07-29T12:51:29.005Z"
   },
   {
    "duration": 18,
    "start_time": "2023-07-29T12:51:33.024Z"
   },
   {
    "duration": 5,
    "start_time": "2023-07-29T12:51:43.229Z"
   },
   {
    "duration": 10,
    "start_time": "2023-07-29T12:51:47.003Z"
   },
   {
    "duration": 141,
    "start_time": "2023-07-29T12:55:05.827Z"
   },
   {
    "duration": 37092,
    "start_time": "2023-07-29T12:56:00.300Z"
   },
   {
    "duration": 54308,
    "start_time": "2023-07-29T12:57:11.637Z"
   },
   {
    "duration": 4,
    "start_time": "2023-07-29T12:58:21.910Z"
   },
   {
    "duration": 538,
    "start_time": "2023-07-29T12:58:26.911Z"
   },
   {
    "duration": 12,
    "start_time": "2023-07-29T12:58:38.988Z"
   },
   {
    "duration": 115,
    "start_time": "2023-07-29T12:58:42.344Z"
   },
   {
    "duration": 39,
    "start_time": "2023-07-29T12:59:06.888Z"
   },
   {
    "duration": 6,
    "start_time": "2023-07-29T12:59:19.112Z"
   },
   {
    "duration": 40624,
    "start_time": "2023-07-29T12:59:26.730Z"
   },
   {
    "duration": 18163,
    "start_time": "2023-07-29T13:00:15.899Z"
   },
   {
    "duration": 3830,
    "start_time": "2023-07-29T13:00:34.064Z"
   },
   {
    "duration": 14088,
    "start_time": "2023-07-29T13:00:37.899Z"
   },
   {
    "duration": 15,
    "start_time": "2023-07-29T13:00:51.997Z"
   },
   {
    "duration": 38,
    "start_time": "2023-07-29T13:00:52.017Z"
   },
   {
    "duration": 3162425,
    "start_time": "2023-07-29T13:00:52.058Z"
   },
   {
    "duration": 8,
    "start_time": "2023-07-29T13:53:34.488Z"
   },
   {
    "duration": 10934,
    "start_time": "2023-07-29T13:53:34.498Z"
   },
   {
    "duration": 52269,
    "start_time": "2023-07-29T13:53:45.434Z"
   },
   {
    "duration": 245981,
    "start_time": "2023-07-29T13:54:37.704Z"
   },
   {
    "duration": 147624,
    "start_time": "2023-07-29T13:58:43.691Z"
   },
   {
    "duration": 67,
    "start_time": "2023-07-29T14:01:11.317Z"
   },
   {
    "duration": 6498643,
    "start_time": "2023-07-29T14:01:11.386Z"
   },
   {
    "duration": 116009,
    "start_time": "2023-07-29T15:49:30.031Z"
   },
   {
    "duration": 48747,
    "start_time": "2023-07-29T15:51:26.042Z"
   },
   {
    "duration": 78,
    "start_time": "2023-07-29T15:52:14.793Z"
   },
   {
    "duration": 452559,
    "start_time": "2023-07-29T17:26:24.185Z"
   }
  ],
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Содержание",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "302.391px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
